# SensFinder 技术实现方案

## 1. 技术架构概述

### 1.1 整体架构
SensFinder采用模块化设计，包含四个主要组件：主控制模块、数据预处理模块、LLM分类模块和结果验证模块。各模块通过清晰的接口进行交互，实现数据流的有序处理。系统支持多种LLM服务提供商，采用适配器模式封装不同LLM服务的调用方式。

### 1.2 技术栈
- **编程语言**：Python 3.8+
- **核心库**：
  - pandas：数据处理和CSV文件读写
  - openai：OpenAI和DeepSeek API调用
  - requests：HTTP请求（用于本地LLM服务）
  - concurrent.futures：多线程并行处理
- **LLM服务**：支持OpenAI、DeepSeek和自定义本地LLM服务

## 2. 模块设计

### 2.1 主控制模块 (sens_finder.py)

**功能**：协调整个处理流程，按顺序调用预处理、分类和验证模块。

**核心组件**：
- `main()`：主函数，定义处理流程
- `run_script()`：动态导入并执行指定模块中的函数
- `print_separator()`：格式化输出信息，提高可读性

**流程控制**：
1. 按顺序执行预处理、分类和验证模块
2. 监控各模块执行状态，遇到错误时中断流程
3. 输出处理结果汇总信息

### 2.2 数据预处理模块 (data_preprocess.py)

**功能**：读取原始文本文件，提取、清洗和划分字段批次。

**核心组件**：
- `preprocess_data()`：预处理主函数

**处理流程**：
1. 创建输出文件夹
2. 递归读取指定目录下的所有文本文件
3. 从文件中提取字段（按行读取并按空格分割）
4. 过滤无效字段（长度检查、至少包含一个字母）
5. 去重处理
6. 按配置的批次大小分割字段并保存为CSV文件

**关键算法**：
- 使用正则表达式`[a-zA-Z]`检查字段是否包含字母
- 使用Python内置的`set()`函数进行快速去重
- 使用pandas库创建DataFrame并导出CSV文件

### 2.3 LLM分类模块 (llm_classify.py)

**功能**：使用LLM服务对字段进行分类，并处理分类结果。

**核心组件**：
- `batch_classify()`：批量分类主函数
- `classify_single_batch()`：处理单个批次
- `load_prompt_template()`：加载提示词模板

**处理流程**：
1. 创建分类结果文件夹
2. 加载提示词模板
3. 获取所有批次文件
4. 使用线程池并行处理每个批次
5. 对每个批次：
   - 读取批次文件
   - 格式化字段文本
   - 根据配置选择并调用LLM服务
   - 解析LLM输出结果
   - 合并原始数据与分类结果
   - 保存分类结果到CSV文件

**LLM服务适配**：
- 使用条件判断选择不同的LLM服务提供商
- 为每种服务实现对应的API调用逻辑
- 标准化响应格式，确保分类结果解析的一致性

**并发处理**：
- 使用`concurrent.futures.ThreadPoolExecutor`创建线程池
- 根据CPU核心数和批次文件数量动态调整线程数
- 监控每个任务的执行状态，收集成功和失败的统计信息

### 2.4 本地LLM客户端 (local_llm.py)

**功能**：提供与本地或自定义LLM服务交互的客户端。

**核心组件**：
- `LocalLLM`类：封装与LLM服务的交互
- `chat()`方法：发送单轮对话请求
- `_send_request()`方法：发送HTTP请求并处理响应

**设计特点**：
- 模拟OpenAI API的响应格式，确保与其他LLM服务的兼容性
- 包含日志记录功能，便于调试和监控
- 支持自定义URL、系统提示和助手提示

### 2.5 结果验证模块 (result_verify.py)

**功能**：验证分类结果，识别问题字段并生成汇总报告。

**核心组件**：
- `verify_results()`：验证主函数

**验证规则**：
1. **规则冲突检测**：检查含公司关键词但分类不是"公司名"的字段
   - 使用正则表达式匹配配置的公司关键词
   - 筛选出分类结果与关键词规则不符的字段

2. **低置信度检测**：识别置信度低于阈值的分类结果
   - 将置信度字段转换为数值类型
   - 筛选出置信度低于配置阈值的字段

**结果处理**：
- 合并不同类型的问题字段，去重处理
- 添加批次来源信息
- 保存为CSV文件，包含源批次、原始文本、分类、置信度、判断依据和问题类型

## 3. 配置管理 (config.py)

**功能**：集中管理系统配置，提供灵活的参数调整。

**配置分类**：
- **路径配置**：各种文件和目录的路径
- **预处理参数**：批次大小、最小字段长度
- **LLM配置**：服务选择、API密钥、模型信息、温度参数
- **验证参数**：低置信度阈值、公司关键词列表

**路径处理**：
- 使用`os.path`模块构建绝对路径，确保跨平台兼容性
- 以项目根目录为基准，构建相对路径

## 4. 提示词工程 (prompt_template.txt)

**设计原则**：
- 明确的分类定义和示例
- 详细的特殊情况处理规则
- 严格的输出格式要求
- 可替换的字段占位符

**分类体系**：
- 人名：自然人的完整姓名或常用名
- 地名：地理区域名称
- 公司名及简称：企业、商业机构的全称或简称
- 组织名及简称：非盈利组织、政府机构等

## 5. 数据流设计

### 5.1 数据转换流程
1. **原始输入** → **清洗后字段** → **分批次字段** → **分类结果** → **问题字段报告**

### 5.2 文件格式
- **输入文件**：纯文本文件，每行一个或多个字段
- **批次文件**：CSV格式，含"raw_text"列
- **分类结果**：CSV格式，含"raw_text"、"category"、"confidence"、"reason"列
- **问题报告**：CSV格式，含"source_batch"、"raw_text"、"category"、"confidence"、"reason"、"problem_type"列

## 6. 错误处理与日志

**错误处理机制**：
- 每个模块都包含异常捕获和处理
- 文件操作失败时提供明确的错误信息和解决方案建议
- API调用失败时显示详细的错误类型和消息
- 主流程中断时显示友好的提示信息

**日志设计**：
- 关键操作节点输出状态信息
- 错误和异常情况输出详细的诊断信息
- 本地LLM客户端使用logging模块记录请求和响应

## 7. 扩展性设计

**模块扩展**：
- 各功能模块相互独立，可单独使用或替换
- 添加新的分类类型只需修改提示词模板和验证规则

**LLM服务扩展**：
- 可轻松添加新的LLM服务提供商
- 只需在`llm_classify.py`中添加对应的API调用逻辑

**验证规则扩展**：
- 可在`result_verify.py`中添加新的验证规则
- 配置文件中的关键词和阈值可根据需求调整

## 8. 安全考虑

- API密钥通过配置文件管理，避免硬编码
- 不存储原始数据，处理完成后只保留必要的结果信息
- 异常处理确保系统在遇到错误时不会泄露敏感信息
- 多线程处理时避免资源竞争和状态不一致问题

## 9. 性能优化

- 使用多线程并行处理批次文件，提高处理效率
- 动态调整线程池大小，避免创建过多线程导致的系统资源浪费
- 使用pandas库进行高效的数据处理和文件读写
- 批处理模式减少LLM API调用次数，降低成本

## 10. 部署与运行

### 10.1 依赖安装
- 使用pip安装必要的Python库：pandas, openai, requests

### 10.2 配置设置
- 编辑`config.py`文件，设置正确的路径、API密钥和参数

### 10.3 运行方式
- 直接运行主脚本：`python script/sens_finder.py`
- 或单独运行各模块进行调试：`python script/data_preprocess.py`等